{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e14773-20a4-4974-8cf6-074ddbe0907f",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "24bafc81-811d-46cb-99ed-d4ac597c69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be69241e-2743-4317-96ec-b97c22713f11",
   "metadata": {},
   "source": [
    "# Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6356dd77-7bab-457b-95df-41733c6d7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "data = pd.read_csv('train.csv')\n",
    "data = np.array(data) # We need just numpy array as we will be doing only algebra\n",
    "m, n = data.shape # Lets store the shape into neat variables\n",
    "np.random.shuffle(data) # Randomise the data before splitting into training and validation set\n",
    "\n",
    "data_dev = data[0:1000].T # Splits the first 1000 digits into validation dataset and transpose it, as Neural Networks work with data for a value in column\n",
    "Y_dev = data_dev[0] # All the labels(digits) are stored here\n",
    "X_dev = data_dev[1:n] # All the pixel value are stored here\n",
    "X_dev = X_dev / 255. # The pixel values are normalised to a activation value between 0 and 1 by dividing it with 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "dc59e8f2-1dff-4f4e-9a98-b49481f0f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same process done with the training dataset\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_, m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed75ab5-212b-4b28-8a1d-297be3ebbcec",
   "metadata": {},
   "source": [
    "# Initializing the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b914128a-dc0d-497d-9c86-89987a3dcf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "def init_params(): # Using a He initialization so that it avoids dying neurons\n",
    "    W1 = np.random.randn(16, 784) * np.sqrt(2 / 784) \n",
    "    b1 = np.zeros((16, 1))\n",
    "    W2 = np.random.randn(16, 16) * np.sqrt(2 / 16)\n",
    "    b2 = np.zeros((16, 1))\n",
    "    W3 = np.random.randn(10, 16) * np.sqrt(2 / 16)\n",
    "    b3 = np.zeros((10, 1))\n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e716a16-2552-41e4-a825-ba44b9265084",
   "metadata": {},
   "source": [
    "# Calculating the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f3671f67-e965-4474-9e17-ea2e7dc5b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z) # Gives the value Z if Z is some positive value, otherwise gives 0\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0 # The derivative gives 1 if Z, as it will be set to True, otherwise gives 0(False).\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z - np.max(Z, axis=0, keepdims=True)) # Provides a stability, the Temp value is set to the highest value Z can get\n",
    "    return A / np.sum(A, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aaee76-e0ea-4170-8da5-d9138eeeeccd",
   "metadata": {},
   "source": [
    "# Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f15ab7a5-0e47-4d68-be63-0de64e1fca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward_prop(W1, b1, W2, b2, W3, b3, X):\n",
    "    Z1 = W1.dot(X) + b1 # Simpe calculation of all the Z and A values\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = ReLU(Z2)\n",
    "    Z3 = W3.dot(A2) + b3\n",
    "    A3 = softmax(Z3) # Only the last layer uses a softmax function with a temp to give a normal distrubition of final activation values\n",
    "    return Z1, A1, Z2, A2, Z3, A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e309b35-74d7-4bf4-b9d9-f4d91908c4ed",
   "metadata": {},
   "source": [
    "# Backward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ec7b8df7-39f7-4566-a22b-fd3421616dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1)) # Creates an array of zeros of 41000 rows and 10 columns(to store the value of digit in activation form) \n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1 # sets the activation of a number to 1 based on the digit(if 2nd digit is 2, then column 2 and row 3 is set to 1)\n",
    "    return one_hot_Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "134f616b-a848-4b85-8a0b-86e96e53109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation\n",
    "def backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y):\n",
    "    m = Y.size # All the mathematics is explained in the report. Please consult.\n",
    "    one_hot_Y = one_hot(Y)\n",
    "\n",
    "    dZ3 = A3 - one_hot_Y\n",
    "    dW3 = (1 / m) * dZ3.dot(A2.T)\n",
    "    db3 = (1 / m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dZ2 = W3.T.dot(dZ3) * ReLU_deriv(Z2)\n",
    "    dW2 = (1 / m) * dZ2.dot(A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    dW1 = (1 / m) * dZ1.dot(X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc866d-931a-44f5-9a84-a2b7fca7abb4",
   "metadata": {},
   "source": [
    "# Updating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "37b70349-11f5-4509-a2c3-a72c91dde184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update parameters\n",
    "def update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n",
    "    W1 -= alpha * dW1 # All the parameters are updated\n",
    "    b1 -= alpha * db1\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2\n",
    "    W3 -= alpha * dW3\n",
    "    b3 -= alpha * db3\n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08f108-4941-421d-bd8b-6f44634c0ad8",
   "metadata": {},
   "source": [
    "# Calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "74b362dd-a4ca-4c0c-91f0-6be23a83559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and accuracy\n",
    "def get_predictions(A3):\n",
    "    return np.argmax(A3, axis=0) # Since a3 is (10, 41000), it returns the index of where in the matrix higeset probabilities are i.e the predicted digit. If index 2, then digit predicted is 2.\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.mean(predictions == Y) # Checks to see how many predictions are same as true value by calculating mean\n",
    "\n",
    "# Training loop\n",
    "def gradient_descent(X, Y, alpha, iterations): # Alpha is set by user\n",
    "    W1, b1, W2, b2, W3, b3 = init_params()\n",
    "    for i in range(iterations): # Trained for a number of iterations set by user\n",
    "        Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y)\n",
    "        W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3,\n",
    "                                               dW1, db1, dW2, db2, dW3, db3, alpha)\n",
    "        if i % 100 == 0: # Prints the accuracy on every 100th iteration\n",
    "            predictions = get_predictions(A3)\n",
    "            acc = get_accuracy(predictions, Y)\n",
    "            print(f\"Iteration {i}: Accuracy = {acc:.4f}\")\n",
    "        elif i == 999:\n",
    "            predictions = get_predictions(A3)\n",
    "            acc = get_accuracy(predictions, Y)\n",
    "            print(f\"Last Iteration: Accuracy = {acc:.4f}\") # Prints the final accuracy\n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "53e5a91c-139e-4517-9b88-cced3a77c34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Accuracy = 0.0740\n",
      "Iteration 100: Accuracy = 0.8611\n",
      "Iteration 200: Accuracy = 0.8974\n",
      "Iteration 300: Accuracy = 0.9097\n",
      "Iteration 400: Accuracy = 0.9177\n",
      "Iteration 500: Accuracy = 0.9234\n",
      "Iteration 600: Accuracy = 0.9287\n",
      "Iteration 700: Accuracy = 0.9323\n",
      "Iteration 800: Accuracy = 0.9361\n",
      "Iteration 900: Accuracy = 0.9391\n",
      "Last Iteration: Accuracy = 0.9402\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "W1, b1, W2, b2, W3, b3 = gradient_descent(X_train, Y_train, alpha=0.1, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc282e-72bd-4128-8e67-71c4235a25bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
